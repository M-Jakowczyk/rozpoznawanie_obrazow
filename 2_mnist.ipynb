{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd2b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f917186",
   "metadata": {},
   "source": [
    "### ZAJĘCIA 8.11.2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c45f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba obrazów w zbiorze treningowym: 60000\n",
      "Liczba obrazów w zbiorze testowym: 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACZCAYAAABHTieHAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGK9JREFUeJzt3XtwVOX9x/EnECCJGAgFbAIZRWOKXCwQITMhgJR4IbbhYmi0IBe1Ih0qTKVVLkJbRaDQobSMDlDuIoqJcodWhYASudViuaURUxRKkNIxJQSSSNjfH4772+8XstkN++zJZt+vGWfOZ8/J2Ufz5Jx93Od7ngiXy+UyAAAAABBgjZxuAAAAAICGicEGAAAAACsYbAAAAACwgsEGAAAAACsYbAAAAACwgsEGAAAAACsYbAAAAACwgsEGAAAAACsYbAAAAACwgsEGAAAAACvCdrBRWVlpnnvuOZOQkGCio6NNamqqeffdd51uFhySn59vIiIirvvP3r17nW4eHMJ1Ahp9Ap64d0A7evSoGTZsmLn99ttNTEyMad26tenbt6/ZtGmT001zTKTTDXDK6NGjTW5urpk4caK58847zYoVK0xmZqbZuXOnSU9Pd7p5cMgzzzxjevbsKV5LSkpyqDVwGtcJaPQJXA/3Dnzr888/N2VlZWbUqFEmISHBXLp0yeTl5ZmsrCyzaNEi89RTTzndxKCLcLlcLqcbEWz79+83qampZu7cuWbSpEnGGGMqKipMly5dTNu2bU1BQYHDLUSw5efnm/79+5u33nrLZGdnO90c1ANcJ6DRJ6Bx74AvqqurTUpKiqmoqDCFhYVONyfownIaVW5urmncuLEYXUZFRZknnnjCfPTRR+bUqVMOtg5OKysrM1euXHG6GXAY1wlo9Al4w70DNWncuLFJTEw0paWlTjfFEWE52Pj73/9ukpOTTWxsrHi9V69exhhjDh065ECrUB+MGTPGxMbGmqioKNO/f39z8OBBp5sEh3CdgEafQE24d0ArLy8358+fN5999pmZP3++2bZtmxkwYIDTzXJEWNZslJSUmPj4+Gte//a1M2fOBLtJcFjTpk3Nww8/bDIzM03r1q3NsWPHzLx580yfPn1MQUGB6d69u9NNRJBxnYBGn4DGvQM1efbZZ82iRYuMMcY0atTIDB061CxcuNDhVjkjLAcbly9fNs2aNbvm9aioKPd+hJe0tDSTlpbmzllZWSY7O9vcfffdZvLkyWb79u0Otg5O4DoBjT4BjXsHajJx4kSTnZ1tzpw5Y9atW2eqq6tNVVWV081yRFhOo4qOjjaVlZXXvF5RUeHeDyQlJZlBgwaZnTt3murqaqebgyDjOgGNPgFfcO+AMcZ07NjRZGRkmJEjR5rNmzebixcvmh/96EcmDJ/LFJ6Djfj4eFNSUnLN69++lpCQEOwmoZ5KTEw0VVVVpry83OmmIMi4TkCjT8BX3DugZWdnmwMHDpiioiKnmxJ0YTnY6NatmykqKjIXLlwQr+/bt8+9HzDGmOLiYhMVFWWaN2/udFMQZFwnoNEn4CvuHdC+nWb5v//9z+GWBF9YDjays7NNdXW1Wbx4sfu1yspKs3z5cpOammoSExMdbB2c8J///Oea1z755BOzceNGc//995tGjcLyTyWscZ2ARp+Axr0D2rlz56557euvvzarVq0y0dHRplOnTg60yllhWSCemppqhg0bZiZPnmzOnTtnkpKSzMqVK83JkyfN0qVLnW4eHJCTk2Oio6NNWlqaadu2rTl27JhZvHixiYmJMbNnz3a6eXAA1wlo9Alo3DugjR071ly4cMH07dvXtGvXzpw9e9asWbPGFBYWmt///vfh+W2XK0xdvnzZNWnSJNd3v/tdV7NmzVw9e/Z0bd++3elmwSELFixw9erVy9WqVStXZGSkKz4+3jVixAjXp59+6nTT4CCuE9DoE/DEvQPa2rVrXRkZGa5bbrnFFRkZ6YqLi3NlZGS4NmzY4HTTHBPhcoVhWTwAAAAA65hMCAAAAMAKBhsAAAAArGCwAQAAAMAKBhsAAAAArGCwAQAAAMAKBhsAAAAArGCwAQAAAMAKn1cQj4iIsNkOWGRrKRX6ROiiT0CjT0Cz0SfoD6GLawQ0X/sE32wAAAAAsILBBgAAAAArGGwAAAAAsILBBgAAAAArGGwAAAAAsILBBgAAAAArGGwAAAAAsILBBgAAAAArGGwAAAAAsILBBgAAAAArGGwAAAAAsILBBgAAAAArGGwAAAAAsILBBgAAAAArGGwAAAAAsCLS6QYAQH1x8803izx16tQajx0+fLjI7du3F3n06NEir1q1SmSXy1WHFgKor5KTk0Xeu3evyHFxcSIfP35c5OnTp4ucm5sbwNYBzuGbDQAAAABWMNgAAAAAYAWDDQAAAABWRLh8nDgcERFhuy0Bdfvtt4us52L7o6ysTOTi4uI6n8sJtuaGh1qfwP+jT1yfrrNYunRpjceWlpaK3LJlS6/nHjx4sMibNm3yo2X20SdunL7PbN26VeT09HSvP5+XlydydnZ2YBpWRzb6RH3rD7GxsSLrv9N169aJXFFRUeO5+vXrJ/KOHTv8asulS5dEfvvtt0WeO3euyEeOHPHr/DeKa8T1tWnTRuSRI0e6tzt37iz23XHHHSIfPXpU5D/+8Y8iFxYWBqKJ1vjaJ/hmAwAAAIAVDDYAAAAAWMFgAwAAAIAVIVuzMWTIEJH79+8vclZWlsiJiYl1fq9Tp06JvHHjRpH1vMz169fX+b1sYJ4lNPrEN5o0aSKyngOdlJTk3p49e7bYd+DAAZHnzZsncocOHUT++OOPRe7Zs6d/jbWMPuG/Ll26iLxr1y6R9boKtamqqhJ5woQJIi9atMiv892ocKjZGDVqlMjLli0Tedq0aSLPmjWrxnPptXZ0zc3ixYtF7tixo8i/+c1vRM7MzBT5q6++EnnFihUir169WuTPPvtM5IsXL16n1b4LlWuE/rs8ceKEyLrupnHjxiL37t1b5O7du4ucmpoqcp8+fUTW/cCTrsvR96DKykqRX3vtNZHHjRtX47mdQM0GAAAAAEcx2AAAAABgBYMNAAAAAFbU25oNvU7Gww8/LPLMmTNF1nPugqm6ulrkKVOmiKyflR3sdTpCZZ4lgoc+8Q09X/af//ynyP/617/c2zk5OWLf+fPnRb7vvvtE1rVdV69eFfmuu+4S+YsvvvChxfbQJ3zj+Ux9PZ9a94EbtXv3bpHvvffegJ6/NuFQs6Hvz4MGDRI5JSVF5EOHDllry0033STy888/L7L+bFGbw4cPizx//nyRV65c6df5QuUaMWzYMJEXLlwo8vvvvy9yu3btRO7bt6/X8+u6i3379onsuT6T/nz4xhtviKzrdvTvxLNu0Bhjvve974ms70PBRs0GAAAAAEcx2AAAAABgBYMNAAAAAFY4VrMRFRUlsp6bqOdHJycnB/T9PZ8/feXKFa/H6voRPc+7Nnoe+Lp160TWz+3Wz1m+UaEyz7I+a9asmch6jQT933jPnj0i33rrrSJ7zvv2RatWrUT+/ve/L7J+vv/+/fu9no8+cX2PPvqoyJ5/u3qdjNqUlJSI3LZtW5FfeeUVkX/+85/7df5Ao0/4xnOO/+DBg62+FzUbgdetWzeR9Xz7yMhIkW+55RaRgzlHXt934uPjRR4xYoTIjz32mMgxMTEi63qTZ599VuSioiKv7QmVa0TTpk1F1uuX6M+XpaWlIv/lL38RWdfZbt++XWS9Fps/GjWS/89fr+c0adIkkR944AGR33333Tq/dyBQswEAAADAUQw2AAAAAFjBYAMAAACAFY7VbOg55/7Oh9Y++eQTkdesWeP1+FdffdW9rZ+ZrJ08eVLkxMRE/xpXi5deeknkGTNmBPT8oTLPMth03cSYMWPc29nZ2WKfXhNBPw+9vLxc5H//+98id+jQQWR/635qo+fi9ujRw+vx9Ilv6PV5cnNzRU5PT3dv67oY3Ue02mo29Dzf2267zev5bKNPfKNFixYi67UzlixZUuOx2pdffinyz372M5H1fUrXMlKzEXjTpk0TWc/nLygoEHnAgAEiV1VV2WlYCOAaEXhdunQRWa+Nou8jCQkJ1tvkD2o2AAAAADiKwQYAAAAAKxhsAAAAALAisvZDAkOvVbF69Wq/fr6srEzkJ598UuStW7eKXFsdhj8mTJggsudz1gNh8uTJIrds2dLr+6Nu9LO1Fy1aJHJsbKx7++LFi2KfrtH46quvRD59+rTIuo/oudl9+vQRWf996Oed6+eGd+3aVeSDBw8a+K9v374iZ2Vliez5TP1f//rXwWgSHDZ27FiR9XPvvTl79qzI999/v8iffvqpyLWt8RToNZdgzMCBA73u13Pkw7lGA9enay51HUVaWpp7W9dkPP744yLrzxba9OnT69LEeodvNgAAAABYwWADAAAAgBUMNgAAAABYEbSajU2bNoncsWNHr8frdQt03YJ+Hr5NH374ocgPPPCA1+NffPFFkTt37iyynqOnn/U/fvx4kanZqBu9bsGf//xnkSMjZfdfsGCBe/vll18W+/S6MHv37hVZ13jU5sSJE34dDzs819G4Hs81do4cORLQ99bXONQP+vrtjZ7fr2s0jh49KvILL7wgcvPmzb2eXx8PIPhGjx4t8k9/+lORPWs0Ak3X9Oq1fRYuXChyfa3z4psNAAAAAFYw2AAAAABgBYMNAAAAAFYErWajU6dOIl+9etXr8XqdgpUrVwa8Tb7673//K/J7773n9Xi9f9WqVSIPHz48MA2DV3q9El0rM3XqVJFnzZpV47lq+50jNGVmZoqs1z3QtTuBpOfaIjj0mjXr168XWT9D35u8vDyRdY1Gs2bNRK7tmfnvvPOOyKyfE3i1ffZISkoSWd83qLUKP23atBG5VatWIhcWFoq8bNky97au9SsoKPD6Xj/4wQ9EnjFjhsjz5s0TWa/to9f0qi/4ZgMAAACAFQw2AAAAAFjBYAMAAACAFUGr2XC5XMF6K8Aner7+3Llz3dt67j4ahoceekjklJQUkc+fPy/yhg0bAvbeFRUVIm/bti1g54bvoqKiRH7wwQf9+nnP39vixYu9Hjtt2jSR9ZpK2scffyxybfUF8N/s2bNF1n/jek2l/Px8kefMmSOy55pLp0+fDkALUd94fja4Xg4kXbe1c+dOkXU9s14/7Pjx4yLra4pT+GYDAAAAgBUMNgAAAABYEbRpVECwHTt2TORdu3aJ3K9fP5E9p9Ts27fPXsPgGP2449qmtfhj4MCBIsfFxYns+ThEY4w5efJkwN4bvktOTr6hn58yZYp7Wz/WUk/R6tKli9dz6Wk3uo8g8LZs2SLyfffdJ7J+VH2PHj1EfvPNN0X2fPSoflSxfpy655QrwBelpaUiP/744yIfPnxY5LVr14rctWtXkauqqgLXOD/wzQYAAAAAKxhsAAAAALCCwQYAAAAAK6jZQL3VokULkSsrK0XWjxLV9NzEX/7ylyK/9957Ir/++uvubf2I1MLCQu+NRUho166d1/16/qs/Jk+eLHKTJk1EPnPmTJ3PjcB58skn/Tr+tddeE7m4uNi9HRMTI/bpx6oOGjTI67mXLFkicklJiV9tw43Tjxbt0KGDyGPGjBE5KytL5Hvuuce9/cMf/lDs0/cR/fsdMWKEyLt37xaZJQOg6Vq/Q4cOiZyeni6yrtn429/+ZqNZteKbDQAAAABWMNgAAAAAYAWDDQAAAABWBK1mQ89nfvnll70eP3ToUJHz8/NFXrFiRSCaFRC33XabyHre5uDBg/06n14fIly9//77Ij///PMi65qL2uhnoOs++ac//cm9PWfOHLGvtrnXaBi2bdvm87F33HGHyHfeeafX4w8cOFCnNuHGREbK29xNN93k9fjt27eL/PTTT4t86dIl93a3bt3EvvHjx/vVtj/84Q9+HQ/7rly5IrKuq9E5MTHRva3rgTIzM0XWa3bs2LFD5JycHJFzc3N9aDHCmf4svXXrVpF79+4tMjUbAAAAABoUBhsAAAAArGCwAQAAAMCKoNVsvPXWWyI/8cQTIuv5z3perZ7b6jlv1hhjtmzZInJ5eXldmnld+nn5rVu3Fnn9+vUi6+ca16a6ulrk3/3ud379fEOVkpIisn7e+QcffCCyXoejNq+++qrI48aNc28/+OCDYl98fLzIPA+/YdB/ex999JHPP9u5c2eR27ZtK3JBQYHI/tYYoW4aNZL/D23ixIkiDx8+3OvPFxUViazvNZ50PUhtdH9D6Dt16pR7e8aMGWKfXndl8+bNIt97770iZ2dnez2+trWlEH6ioqK87v/HP/4RpJZ4xzcbAAAAAKxgsAEAAADACgYbAAAAAKwIWs1GcXGxyG+88YbIU6dO9frzN998s8hr164VWc9LW7Nmjb9NrJF+fr5+lvaN0s9JXr16dUDPH6reeecdkR999FGRPZ9vbowxv/rVr0TWz5PW86X1HPuIiAj3tq7TSUtLEzkvL6+mZqMeueeee0Ru2bKlyLpP7Nu3z+dzZ2RkeN2v14lhvn5w9OrVS2R/a+Bqq6156qmn3Nv6mlObWbNmiVxWVubXzyO0XL58WeSxY8eKvH//fpGHDRsm8iuvvCLy7t27A9g6NARTpkzxul/3MafwzQYAAAAAKxhsAAAAALCCwQYAAAAAK4JWs6GtWrVKZL2GQkJCgl/nu/vuu71mJxUWFor85ptvijxnzpxgNidkPPbYYyIvX75c5EGDBoms1zXQdUJ6/mz79u1FbtGihXv76tWrYt/p06d9aDHqG11v1bx5c5Grqqp8Pld6errIOTk5Xo/X67igfnrppZdE1ms2JSUliTxt2jT3tr6GaLt27RJ55syZdWkiLIqOjhZZ3ycC6cSJE16zXltK3wOp2YBeV0OvUafrmevL2ix8swEAAADACgYbAAAAAKxgsAEAAADACsdqNvRcxSFDhoisn1EfExMjcqNG9WecdOXKFZGPHz8u8uDBg0U+efKk5RY1DOXl5SL/+Mc/Frlbt24iP/PMMyIPHTpU5NjYWJF1XYbn+y1ZskTs82f9BYQOvZ6Kvg799a9/dW/r2qrWrVuLvHTpUpFLS0sD0EL467nnnvO6/+LFiyLPnz9f5FGjRomsf+9t2rSp8dxnz54VWV/7KysrvbYNwbdjxw6Rf/vb34q8bdu2Op9bf07RdV56vr22Z8+eOr83apaVlSXyuXPnRN67d28wm+OXp59+WuS4uDiR9WcX/TnHKfXnEzsAAACABoXBBgAAAAArGGwAAAAAsCLC5XK5fDowIsJ2W7yaMGGCyPr50927d7f23l988YXIGzduFFnP+dywYYO1ttSFj79ivzndJ2qj63w6deoksn6euufvuayszF7D6oFw6RN6bQw9/1r3kdzcXJHvuusu93bnzp3Fvg8++EDkgQMHimzzef02NJQ+ode26NOnj8gXLlwQWdd+/eIXvxB5/PjxNb5XUVGRyI888ojIhw4d8tbUes9Gn6hv1whdW6XrLKZPny6ynhPvKT4+XmTPNVmMufZzi/b222+LPHLkSJGdvqaEyjWiX79+Iufl5Yms11bp3bu3yE7+3ep7UteuXUXW9x29Zt24ceNE/vrrrwPYumv52if4ZgMAAACAFQw2AAAAAFjBYAMAAACAFSFTs6HpuZHf+c53rL2Xfi57qK2TESrzLBE84donPv/8c5Hbt2/v889++eWXIutntR88eLDuDasHGkqfGD16tMjLli0TWa+LpK/nSUlJXs9fXFzs3p45c6bYt3z5ch9bGRrCoWZjwIABIi9cuFDk5ORka+99+PBhkX/yk5+IfOzYMWvvXRehco146KGHRN68ebPX448cOSLyggULRN6yZYvIJSUlN9A6qWnTpiLrdV70ukH5+fki6/tQsOtNqdkAAAAA4CgGGwAAAACsYLABAAAAwIqQrdmA70JlniWCJ1z7RP/+/UXW6240adJEZM+5uvp5+6G+hoLWUPpEmzZtRN66davIKSkpfp2vsrJS5J49e7q39VzvhiYcaja0li1bivzCCy+IPGTIEJFvvfVW9/aZM2fEvvXr14us1+g4ceKEyJcuXfKnqUEXKtcIfR3Xa0/k5OSI3K5dO5E9f6fGGFNVVSVydXW11/ffs2ePe1v/u6WlpYms9+uaMl0LmJGR4VdbbKNmAwAAAICjGGwAAAAAsILBBgAAAAArqNkIA6EyzxLBQ5+A1lD7RF5ensh6zr324YcfijxnzhyR9TP3G7JwrNlAzRrqNSIuLk7kRx55ROQePXqIrOt6srOz6/zer7/+usgvvviiyIWFhXU+dzBQswEAAADAUQw2AAAAAFjBYAMAAACAFdRshIGGOs8SdUefgEafgEbNBjxxjYBGzQYAAAAARzHYAAAAAGAFgw0AAAAAVjDYAAAAAGAFgw0AAAAAVjDYAAAAAGAFgw0AAAAAVjDYAAAAAGAFgw0AAAAAVjDYAAAAAGAFgw0AAAAAVkS4XC6X040AAAAA0PDwzQYAAAAAKxhsAAAAALCCwQYAAAAAKxhsAAAAALCCwQYAAAAAKxhsAAAAALCCwQYAAAAAKxhsAAAAALCCwQYAAAAAK/4P5w1eQO9TAlUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Załaduj zbiór MNIST (train/test) i pokaż kilka przykładów\n",
    "\n",
    "root = \"./data\"\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=root, train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "print(f\"Liczba obrazów w zbiorze treningowym: {len(train_dataset)}\")\n",
    "print(f\"Liczba obrazów w zbiorze testowym: {len(test_dataset)}\")\n",
    "\n",
    "# pokaż pierwsze 6 obrazów z treningowego loadera\n",
    "images, labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10, 2))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n",
    "    axes[i].set_title(int(labels[i]))\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec348e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_size: int = 28*28, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Akceptuje zflattenowane wejście (batch, input_size) lub obraz (batch, C, H, W)\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # po dwóch warstwach conv i dwóch poolingach z 28x28 -> 7x7, kanały 64\n",
    "        self.fc2 = torch.nn.Linear(64 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Oczekuje wejścia obrazu (batch, C, H, W)\n",
    "        # Jeśli przyjdzie zflattenowane, spróbuj przywrócić (N, 1, 28, 28)\n",
    "        if x.dim() == 2:\n",
    "            x = x.view(x.size(0), 1, 28, 28)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc2(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1041d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check - batch loss: 2.3073; logits shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Sanity check jednego batcha\n",
    "images_batch, labels_batch = next(iter(train_loader))\n",
    "images_batch = images_batch.to(device)\n",
    "labels_batch = labels_batch.to(device)\n",
    "if isinstance(model, SimpleNN):\n",
    "    x_batch = images_batch.view(images_batch.size(0), -1)\n",
    "else:\n",
    "    x_batch = images_batch\n",
    "logits_batch = model(x_batch)\n",
    "loss_batch = criterion(logits_batch, labels_batch)\n",
    "print(f\"Sanity check - batch loss: {loss_batch.item():.4f}; logits shape: {logits_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa9b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 2.1017 - Train acc: 0.5458\n"
     ]
    }
   ],
   "source": [
    "# Trening modelu na zbiorze train_loader\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    for imgs, labs in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labs = labs.to(device)\n",
    "\n",
    "        # Dla SimpleNN spłaszczamy, dla ConvNN zostawiamy obraz\n",
    "        if isinstance(model, SimpleNN):\n",
    "            x = imgs.view(imgs.size(0), -1)\n",
    "        else:\n",
    "            x = imgs\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, labs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        running_correct += (preds == labs).sum().item()\n",
    "        running_total += imgs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "    print(f\"Epoch {epoch}/{epochs} - Loss: {epoch_loss:.4f} - Train acc: {epoch_acc:.4f}\")\n",
    "\n",
    "# Ewaluacja na zbiorze testowym\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labs in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labs = labs.to(device)\n",
    "        if isinstance(model, SimpleNN):\n",
    "            x = imgs.view(imgs.size(0), -1)\n",
    "        else:\n",
    "            x = imgs\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        test_correct += (preds == labs).sum().item()\n",
    "        test_total += imgs.size(0)\n",
    "\n",
    "print(f\"Test accuracy: {test_correct / test_total:.4f}\")\n",
    "\n",
    "# Zapisz model z nazwą zależną od typu\n",
    "# model_filename = \"simple_nn_mnist.pth\" if isinstance(model, SimpleNN) else \"conv_nn_mnist.pth\"\n",
    "# torch.save(model.state_dict(), model_filename)\n",
    "# print(f\"Model zapisany do {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
